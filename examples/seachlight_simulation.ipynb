{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Layered Searchlight Simulation\n",
    "\n",
    "This notebook uses the layered searchlight model to analyze a simulated spatial dataset, specified in `examples/bin/simulation_dataloader.py`. The dataloader for the full MTurk1 project data is a drop in replacement for this simulation dataloader."
   ],
   "id": "c8d190efa0c33fcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from neurotools import decoding, embed, geometry, util\n",
    "from bin.simulation_dataloader import SimulationDataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\"\n"
   ],
   "id": "3192bfcc88c6b455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We define a data loader that will embed information allowing us to distiguish between 12 classes at 3 locations in a 16 x 16 x 16 space. This 16x16x16 space will be called the \"full space\" and is analogous to the voxel space in MRI data. The 12 classes are layed out in a circle on a low dimension manifold in this space, plus some degree of random noise. We plot an example of what this low dimension embedding of the classes might look like in a 3D feature space, and then the locations of class representaions in a 2D slice of the full space. Locations where there are only representations of set A are shown in teal, and locations where there are joint representaions of set A and B are shown in yellow."
   ],
   "id": "bed68b722e5356a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We expect to be able to decode A in all three of the above locations. A classifier trained on A should be able to also decode examples drawn from B in the bottom right yellow colored region."
   ],
   "id": "ea50e6fd4ad22276"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to operate in as close a setting to the full fMRI decoding task as possible, we also impose the constraint that only some classes should be directly compared to each other. This is generally because only data for some classes was collected under comparable conditions. We define allowed comparisons via a pairwise matrix, with ones at the index of allowed comparisons, and zeroes elsewhere."
   ],
   "id": "5e3f2b361ab3ca0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create pairwise:\n",
    "main_light = [1, 3, 5]\n",
    "other_light = [2, 4, 6]\n",
    "main_dark = [7, 9, 11]\n",
    "other_dark = [8, 10, 12]\n",
    "# we only want comparisons within a set and within luminance levels\n",
    "set_wieghts = []\n",
    "pair_weights = torch.empty((12, 12, 12))\n",
    "for item_set in [main_light, other_light, main_dark, other_dark]:\n",
    "    rows = torch.zeros((12, 12))\n",
    "    cols = torch.zeros((12, 12))\n",
    "    ind = torch.tensor(item_set) - 1\n",
    "    cols[:, ind] = 1\n",
    "    rows[ind, :] = 1\n",
    "    weights = torch.logical_and(cols, rows).float()\n",
    "    for t in item_set:\n",
    "        pair_weights[t - 1] = weights\n",
    "pm = pair_weights"
   ],
   "id": "a8889e77431a40ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another feature of the searchlight decoder is that it allows us to pool the results of the searchlight at different subregions of the full input. THis is useful for generating quantative measures of stats like accuracy in specific regions. We can efine regions via an atlas which is constructed below. We plot a slice through the atlas, with purple background, blue roi_1, cyan roi_2, green roi_3, and yellow roi_4. We expect cross decoding of B from A only in roi_2."
   ],
   "id": "1d77ed24b387d65f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "atlas = np.zeros((16, 16, 16))\n",
    "atlas[:8, :12, :] = 1 # Blue\n",
    "atlas[8:, 4:, :] = 2 # cyan\n",
    "lookup = {1: \"roi_1\", 2:\"roi_2\"}\n",
    "plt.imshow(atlas[:, :, 8])"
   ],
   "id": "bc131d4da56b5a19",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "n_classes = 12\n",
    "vdl = SimulationDataloader(difficulty=0, seed=8, num_examples=100, batch_size=70)\n",
    "vdl.plot_circle_embedding()\n",
    "vdl.plot_templates()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d122b8287251468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we need to define some variables to initialize the layered searchlight model!\n",
    "- BASE_KERNEL_SIZE is the dimensions of the cube of data sampled by each filter in each layer.\n",
    "- N_LAYERS is the total number of layers.\n",
    "Note that the amount of input data availble to each searchlight spot is a cube of size is fully determined by theses two parameters. In this case, each searchlight spot \"sees\" a 5x5x5 cube of input data."
   ],
   "id": "4de77fbe01b7f702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BASE_KERNEL_SIZE = 2\n",
    "N_LAYERS = 4\n",
    "xdecoder = decoding.ROISearchlightDecoder(atlas, lookup, set_names=(\"a\", \"b\"), in_channels=1, n_classes=12, spatial=(16, 16, 16), nonlinear=True, device=dev, base_kernel_size=BASE_KERNEL_SIZE, n_layers=N_LAYERS, dropout_prob=0.5, seed=8, share_conv=False, pairwise_comp=pm)\n",
    "print(\"Total model parameters:\", xdecoder.get_model_size())"
   ],
   "id": "5bdf9c51e063a224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we'll initialize our simulation dataloader for training the layered searchlight. We'll set the difficulty parameter to 2, which adds  noise and affine jitter to the classes. This is a very low level of noise, so classification will be fairly easy. The data loader is set so calling `vdl[0]` will give an iterator over batches of examples from set A, and calling `vdl[1]` will give an iterator on set B. We will train on A then later test cross decoding on B"
   ],
   "id": "a0fe7a91e20e357f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# scaling factor on amount of noise and degree of affine jitter\n",
    "difficulty = 2\n",
    "vdl= SimulationDataloader(difficulty=difficulty, seed=8, num_examples=120, batch_size=70)"
   ],
   "id": "77bd2b34f62b7283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Searchlight decoder's `fit` method expects an iterator the returns a tuple of numpy arrays `(stimulus: <batch, channels, width, height, depth>, target_class: <batch>)`, this is the form of the iterators returned from the SimulationDataloader object (called `vdl` above).\n",
    "\n",
    "The xdecoder train searchligh and train predictor state variable can be set to deterine the training routine. Training the searchlight means attempting to predict class identity as well as possible at every location in the full space. Training the predictor sets the weights on each searchlight spot based on the cofidence of it's predictions. These weights will be important for determining accuracy and getting representaional geometries over whole rois. Generally, the searchlight should be trained first, though they could also be set in tandem."
   ],
   "id": "c887c15ec12c69a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xdecoder.train_searchlight(\"a\", True)\n",
    "xdecoder.train_predictors(\"a\", False)\n",
    "s_lh = xdecoder.fit(vdl.batch_iterator(\"A\", 1500), lr=.02)\n",
    "#\n",
    "xdecoder.train_searchlight(\"a\", False)\n",
    "xdecoder.train_predictors(\"a\", True)\n",
    "p_lh = xdecoder.fit(vdl.batch_iterator(\"A\", 1000), lr=.02)\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].plot(s_lh)\n",
    "ax[1].plot(p_lh)"
   ],
   "id": "360bbee6c8b24007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model prints the sum of the difficulty reweighted cross entropy loss at every batch, only with the value of the regularization and the average accuracy over all searchlight spots."
   ],
   "id": "fc18a58ef622ca57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we have a trained model. First we look at the confidence (i.e. the weights) over the full space. Note that they align with where there is signal from set A."
   ],
   "id": "1fc05fc22927757a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sal_map = xdecoder.get_saliancy()\n",
    "plt.imshow(sal_map[:, :, 8], vmin=0, vmax=.07)"
   ],
   "id": "96dfd0f0568b42d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By running the searchlight's predict method, we will get measures of maps of searchlight accuracy over the whole space on a validation set, as well as quantifications of accuracy in each ROI and combined over the full space. We can initialize a simulation dataloader with a new seed to get new examples for the validation set."
   ],
   "id": "852644f9534f19d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vdl = SimulationDataloader(difficulty=difficulty, seed=16, num_examples=100, batch_size=70)\n",
    "roi_accs, acc_map, gradient_map = xdecoder.predict(vdl.batch_iterator(\"A\", 50))\n",
    "print(roi_accs)"
   ],
   "id": "99676a04d0b43851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can now plot the accuracy map over the space from the searchlight"
   ],
   "id": "5110725553215270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.imshow(acc_map[:, :, 6:10].mean(axis=2), vmin=.33, vmax=.55)\n",
    "print(\"Min Acc:\", np.min(acc_map).squeeze())\n",
    "print(\"Max Acc:\", np.max(acc_map).squeeze())"
   ],
   "id": "179f19308137206a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So were pretty great at predicting the exemplars of set A on a validation set. But what about cross decoding? Let's try predicting on exemplars of set B by accessing `vdl[1]`"
   ],
   "id": "b0d9200b4239308"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "roi_accs, acc_map, gradient_map = xdecoder.predict(vdl.batch_iterator(\"B\", 50))\n",
    "print(roi_accs)"
   ],
   "id": "2df00ab2913a9d7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Performance is a bit worse, and we can see is only greater than chance in roi 2, the region that overlaps the bottom right hand side. Let's look at the map of cross decoding, and see that it looks as expected"
   ],
   "id": "4f14483a81257ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.imshow(acc_map[:, :, 6:10].mean(axis=2), vmin=1/3, vmax=.46)\n",
    "print(\"Min Acc:\", np.min(acc_map).squeeze())\n",
    "print(\"Max Acc:\", np.max(acc_map).squeeze())"
   ],
   "id": "a5032dfba636ab2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So now we know that we can train a model on Set A and accurately classify in three regions that have significant Set A signal. We can then attempt to cross decode set B with the same trained model, and show that exemplars of Set B can be cross decoded from Set A in one of these regions.\n",
    "\n",
    "Our final question looks at wether we can recover the intial geometry of the class embeddings from the models latent space. Calling the searchlight's `get_latent` method will give us a cosine distance matrix between the classes for each searchlight spot, and also give the distance matrices over each ROI, weighted by the confidence at each spot. We can do this for both Set A and Set B"
   ],
   "id": "4127bebc561f31a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rdm, roi_rdm = xdecoder.get_latent(vdl.batch_iterator(\"B\", 50), metric=\"pearson\", voxelwise=True)"
   ],
   "id": "fbc38fc7bbcb3bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The rdm is returned as the upper triangle of the distance matrix between the 12 classes. However, we know a ground truth geometry (a circle) only for to sets of 6 classes, so we need to break up the rdm with some utility functions. For now, we will focus on the RDM for roi 2, where we saw significant cross decoding."
   ],
   "id": "1499ec08324c16a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "r_rdm = roi_rdm[\"roi_1\"]\n",
    "square_rdm = util.triu_to_square(torch.from_numpy(r_rdm), 12).numpy()\n",
    "square_rdm = square_rdm[:, 6:, 6:].squeeze()\n",
    "triu_ind = np.triu_indices(6, 1)\n",
    "sub_r_rdm = square_rdm[triu_ind]\n",
    "print(sub_r_rdm)"
   ],
   "id": "7e3c2e81375345ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can now initial an MDS algorithm to determine the optimal geometry given the RDM.\n",
    "\n",
    "_Note: depending on random initialization, MDS may rarely plateau in some local minima, if loss stabilizes at a value greater than about 10000, you should rerun_"
   ],
   "id": "fc3d4a10942b2442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mds = embed.MDScale(n=6, embed_dims=2, initialization=\"pca\")\n",
    "embedding = mds.fit_transform(sub_r_rdm)"
   ],
   "id": "e25b01da6d343094",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After the MDS has finished we can plot the embedding, with some color coding indicating expected order."
   ],
   "id": "11e6c8a84f393395"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colors = [\"#eb8ba7\", \"#caa65a\", \"#8dba7b\", \"#56bccd\", \"#9da3fe\", \"#e385f0\"]\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=colors)"
   ],
   "id": "b9819336ce6a8d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is the expected order! We can also quantify this by taking the pearson correlation of the rank order of the distances between the points in an ideal circle (accounting for ties) with the rank order of the distances from the model. (i.e. the tie corrected spearman correlation of the distance matrices)"
   ],
   "id": "c2a5d66006078c9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if type(sub_r_rdm) is np.ndarray:\n",
    "    sub_r_rdm = torch.from_numpy(sub_r_rdm)\n",
    "rho  = geometry.circle_corr(sub_r_rdm.unsqueeze(0), 6, metric=\"rho\")"
   ],
   "id": "c7f4a42f42371503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This correlation is high, indication that the rdm from the model strongly agrees with the expected circular ordering.",
   "id": "241763f6e9fbef40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The error bars of this rho tell us our confidence in the value given the model. However, we need to consider the possibility that a dissimilarity structure that matches color space could occur at random. The simulation below establish the upper 95% quantile of the distribution of random rankings with the color space ranking for both the light and dark samples.",
   "id": "a94dc5c9b5209938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(rho)\n",
    "samp = []\n",
    "for i in range(1000):\n",
    "    n = 2\n",
    "    s1 = [geometry.circle_corr(torch.rand_like(sub_r_rdm).unsqueeze(0), 6, metric=\"rho\").detach().numpy() for _ in range(n)]\n",
    "    samp.append(np.stack(s1).sum(axis=0) / n)\n",
    "samp = np.stack(samp)\n",
    "print(samp.mean())\n",
    "print(np.quantile(samp, .95))"
   ],
   "id": "e67ed9d98d1724ce",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
